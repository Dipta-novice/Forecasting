{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "id": "RXS8LWpjw1cH"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU, TimeDistributed, Input\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.model_selection import cross_validate,GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.utils.multiclass import unique_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "id": "2me0T-hMw1cJ"
      },
      "cell_type": "markdown",
      "source": [
        "# Load datasets and cleaning\n",
        "train: 2016-01-01 : 2017-04-22\n",
        "\n",
        "test: 2017-04-23 : 2017-05-31"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-FewN-zJw1cM"
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train_final.csv\")\n",
        "test = pd.read_csv(\"test_final.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yTQcormrw1cN"
      },
      "cell_type": "code",
      "source": [
        "# Here I got rid of some features that were considered having duplicated contribution to the model training\n",
        "train_df = train_df.drop(columns=[ 'population', 'reserve_visitors', 'days_diff', 'day', 'season'])\n",
        "test = test.drop(columns=['population', 'reserve_visitors','days_diff', 'day', 'season'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "L3OXhgkxw1cN"
      },
      "cell_type": "code",
      "source": [
        "# refine column names\n",
        "train_df = train_df.rename({'visitors_x': 'visitors'}, axis = 1)\n",
        "train_df = train_df.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n",
        "train_df = train_df.rename({'month_y': 'month'}, axis = 1)\n",
        "train_df = train_df.rename({'longitude_y': 'longitude'}, axis = 1)\n",
        "train_df = train_df.rename({'latitude_y': 'latitude'}, axis = 1)\n",
        "test = test.rename({'latitude_y': 'latitude'}, axis = 1)\n",
        "test = test.rename({'longitude_y': 'longitude'}, axis = 1)\n",
        "test = test.rename({'month_y': 'month'}, axis = 1)\n",
        "test = test.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PDGoIo79w1cO"
      },
      "cell_type": "code",
      "source": [
        "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
        "train_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fSyFeWYRw1cO"
      },
      "cell_type": "code",
      "source": [
        "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
        "test.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uar5SCZ_w1cP"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode categorical columns\n",
        "Categorical columns: 'Food_Type','season', 'day_of_week', 'air_store_id'\n",
        "\n",
        "One-hot encoding may provide better result, but I applied labels encoding to avoid high dimensional feature space."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7A1uYp7gw1cP"
      },
      "cell_type": "code",
      "source": [
        "# Weekday\n",
        "le_weekday = LabelEncoder()\n",
        "le_weekday.fit(train_df['day_of_week'])\n",
        "train_df['day_of_week'] = le_weekday.transform(train_df['day_of_week'])\n",
        "test['day_of_week'] = le_weekday.transform(test['day_of_week'])\n",
        "\n",
        "# id\n",
        "le_id = LabelEncoder()\n",
        "le_id.fit(train_df['air_store_id'])\n",
        "train_df['air_store_id'] = le_id.transform(train_df['air_store_id'])\n",
        "test['air_store_id'] = le_id.transform(test['air_store_id'])\n",
        "\n",
        "# food type\n",
        "le_ftype = LabelEncoder()\n",
        "le_ftype.fit(train_df['Food_Type'])\n",
        "train_df['Food_Type'] = le_ftype.transform(train_df['Food_Type'])\n",
        "test['Food_Type'] = le_ftype.transform(test['Food_Type'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNWsWJ41w1cR"
      },
      "cell_type": "markdown",
      "source": [
        "### Fill the cells of missing values"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4TYFYN3Jw1cR"
      },
      "cell_type": "code",
      "source": [
        "train_df = train_df.fillna(-1)\n",
        "test = test.fillna(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYKMaYK3w1cR"
      },
      "cell_type": "markdown",
      "source": [
        "# Input preparation for Seq2Seq modeling\n",
        "\n",
        "Time-independent features (autocorrelations, country, etc) are \"stretched\" to timeseries length."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "atLZUdcow1cS"
      },
      "cell_type": "code",
      "source": [
        "# combine train and test sets\n",
        "X_all = train_df.append(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4FAyV9n-w1cS"
      },
      "cell_type": "code",
      "source": [
        "# date table (includes all dates for training and test period)\n",
        "dates = np.arange(np.datetime64(X_all.visit_date.min()),\n",
        "                  np.datetime64(X_all.visit_date.max()) + 1,\n",
        "                  datetime.timedelta(days=1))\n",
        "ids = X_all['air_store_id'].unique()\n",
        "dates_all = dates.tolist()*len(ids)\n",
        "ids_all = np.repeat(ids, len(dates.tolist())).tolist()\n",
        "df_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\n",
        "df_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7T0PSbgw1cT"
      },
      "cell_type": "markdown",
      "source": [
        "Data relevant to 'visit_date'"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gOfZNzdYw1cT"
      },
      "cell_type": "code",
      "source": [
        "# create copy of X_all with data relevant to 'visit_date'\n",
        "X_dates = X_all[['visit_date', 'year','month','week', 'is_holiday','next_day','prev_day',\\\n",
        "                 'daysToPrev25th','day_of_week','Consecutive_holidays']].copy()\n",
        "\n",
        "# remove duplicates to avoid memory issues\n",
        "X_dates = X_dates.drop_duplicates('visit_date')\n",
        "\n",
        "# merge dataframe that represents all dates per each restaurant with information about each date\n",
        "df_to_reshape = df_all.merge(X_dates,\n",
        "                             how = \"left\",\n",
        "                             left_on = 'visit_date',\n",
        "                             right_on = 'visit_date')\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-cmydpFsw1cU"
      },
      "cell_type": "markdown",
      "source": [
        "Data relevant to 'air_store_id'"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZgKl6ta7w1cU"
      },
      "cell_type": "code",
      "source": [
        "# create copy of X_all with data relevant to 'air_store_id'\n",
        "X_stores = X_all[['air_store_id', 'Food_Type', 'latitude','longitude']].copy()       \n",
        "\n",
        "# remove duplicates to avoid memory issues\n",
        "X_stores = X_stores.drop_duplicates('air_store_id')\n",
        "\n",
        "# merge dataframe that represents all dates per each restaurant with information about each restaurant\n",
        "df_to_reshape = df_to_reshape.merge(X_stores,\n",
        "                                    how = \"left\",\n",
        "                                    left_on = 'air_store_id',\n",
        "                                    right_on = 'air_store_id')\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKPv9Wo_w1cU"
      },
      "cell_type": "markdown",
      "source": [
        "Data relevant to 'air_store_id'"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2JF1TBa_w1cV"
      },
      "cell_type": "code",
      "source": [
        "# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\n",
        "df_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date','prev_visitors', 'mean_visitors', \n",
        "                                       'median_visitors', 'max_visitors', 'min_visitors', 'count_observations','visitors']],\n",
        "                                    how = \"left\",\n",
        "                                    left_on = ['air_store_id', 'visit_date'],\n",
        "                                    right_on = ['air_store_id', 'visit_date'])\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "b9pc2GT-w1cV"
      },
      "cell_type": "code",
      "source": [
        "# separate 'visitors' into output array\n",
        "Y_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n",
        "\n",
        "# take log(y+1)\n",
        "Y_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n",
        "\n",
        "# add flag for days when a restaurant was closed\n",
        "df_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n",
        "                                        df_to_reshape['visit_date'].isin(train_df['visit_date']).values,1,0)\n",
        "\n",
        "# drop 'visitors' and from dataset\n",
        "df_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n",
        "\n",
        "# fill in NaN values\n",
        "df_to_reshape = df_to_reshape.fillna(-1)\n",
        "\n",
        "# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\n",
        "columns_list = [x for x in list(df_to_reshape.iloc[:,2:])]\n",
        "\n",
        "# bound all numerical values between -1 and 1\n",
        "# note: to avoid data leakage 'fit' should be made on traid data and 'transform' on train and test data\n",
        "# in this case all data in test set is taken from train set, thus fit/transform on all data \n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_to_reshape[columns_list])\n",
        "df_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "me_yd_llw1cV"
      },
      "cell_type": "code",
      "source": [
        "# SPECIFIC PREPARATION FOR NEURAL NETWORK AND ENCODER/DECODER ---------------\n",
        "# reshape X into (samples, timesteps, features)\n",
        "X_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n",
        "                                                len(dates),\n",
        "                                                df_to_reshape.shape[1]-2)\n",
        "\n",
        "# isolate output for train set and reshape it for time series\n",
        "Y_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(train_df['visit_date'].values) &\n",
        "                          Y_lstm_df['air_store_id'].isin(train_df['air_store_id'].values),]\n",
        "Y_lstm = Y_lstm_df.values[:,2].reshape(len(train_df['air_store_id'].unique()),\n",
        "                                       len(train_df['visit_date'].unique()),\n",
        "                                       1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2-XAC-YFw1cV"
      },
      "cell_type": "code",
      "source": [
        "# test dates\n",
        "n_test_dates = len(test['visit_date'].unique())\n",
        "\n",
        "# make additional features for number of visitors in t-1, t-2, ... t-7\n",
        "t_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\n",
        "for i in range(1,8):\n",
        "    temp = Y_lstm.copy()\n",
        "    temp[:,i:,:] = Y_lstm[:,0:-i,:].copy()\n",
        "    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\n",
        "t_minus = t_minus[:,:,1:]\n",
        "print (\"t_minus shape\", t_minus.shape)\n",
        "\n",
        "\n",
        "# split X_all into training and test data\n",
        "X_lstm = X_all_lstm[:,:-n_test_dates,:]\n",
        "X_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n",
        "\n",
        "# add t-1, t-2 ... t-7 visitors to feature vector\n",
        "X_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)\n",
        "\n",
        "# split training set into train and validation sets\n",
        "X_tr = X_lstm[:,39:-140,:]\n",
        "Y_tr = Y_lstm[:,39:-140,:]\n",
        "\n",
        "X_val = X_lstm[:,-140:,:]\n",
        "Y_val = Y_lstm[:,-140:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9CbuXptFw1cW"
      },
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoder Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_MywX23Uw1cX"
      },
      "cell_type": "code",
      "source": [
        "# MODEL FOR ENCODER AND DECODER -------------------------------------------\n",
        "num_encoder_tokens = X_lstm.shape[2]\n",
        "latent_dim = 256 \n",
        "\n",
        "# encoder training\n",
        "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, \n",
        "               batch_input_shape = (1, None, num_encoder_tokens),\n",
        "               stateful = False,\n",
        "               return_sequences = True,\n",
        "               return_state = True,\n",
        "               recurrent_initializer = 'glorot_uniform')\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c] \n",
        "\n",
        "# Decoder training, using 'encoder_states' as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "decoder_lstm_1 = LSTM(latent_dim,\n",
        "                      batch_input_shape = (1, None, num_encoder_tokens),\n",
        "                      stateful = False,\n",
        "                      return_sequences = True,\n",
        "                      return_state = False,\n",
        "                      dropout = 0.4,\n",
        "                      recurrent_dropout = 0.4) # True\n",
        "\n",
        "decoder_lstm_2 = LSTM(128, \n",
        "                     stateful = False,\n",
        "                     return_sequences = True,\n",
        "                     return_state = True,\n",
        "                     dropout = 0.4,\n",
        "                     recurrent_dropout = 0.4)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm_2(decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\n",
        "decoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# Train the model\n",
        "training_model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-6af_OfGw1cX"
      },
      "cell_type": "code",
      "source": [
        "# useful for understanding the model architecture\n",
        "training_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FDwtQlqIw1cX"
      },
      "cell_type": "code",
      "source": [
        "# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n",
        "# generator that randomly creates times series of 39 consecutive days\n",
        "# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \n",
        "def dec_enc_n_days_gen(X_3d, Y_3d, length):\n",
        "    while 1:\n",
        "        decoder_boundary = X_3d.shape[1] - length - 1\n",
        "        \n",
        "        encoder_start = np.random.randint(0, decoder_boundary)\n",
        "        encoder_end = encoder_start + length\n",
        "        \n",
        "        decoder_start = encoder_start + 1\n",
        "        decoder_end = encoder_end + 1\n",
        "        \n",
        "        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n",
        "        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n",
        "        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n",
        "        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n",
        "        \n",
        "        yield([X_to_conc,\n",
        "               X_to_decode],\n",
        "               Y_decoder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0HN07zDAw1cX"
      },
      "cell_type": "code",
      "source": [
        "# TRAINING -------------------------------------------------------------\n",
        "# Training on X_tr/Y_tr and validate with X_val/Y_val\n",
        "# To perform validation training on validation data should be\n",
        "# made instead of training on full data set.\n",
        "# Then validation check is made on period outside of training data\n",
        "# (included in code below).\n",
        "\n",
        "training_model.fit_generator(dec_enc_n_days_gen(X_tr, Y_tr, 39),\n",
        "                             validation_data = dec_enc_n_days_gen(X_val, Y_val, 39),\n",
        "                             steps_per_epoch = X_lstm.shape[0],\n",
        "                             validation_steps = X_val.shape[0],\n",
        "                             verbose = 1,\n",
        "                             epochs = 5)\n",
        "\n",
        "\n",
        "# # Training on full dataset\n",
        "# training_model.fit_generator(dec_enc_n_days_gen(X_lstm[:,:,:], Y_lstm[:,:,:], 39),\n",
        "#                             steps_per_epoch = X_lstm[:,:,:].shape[0],\n",
        "#                             verbose = 1,\n",
        "#                             epochs = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_vq_FWDZw1cY"
      },
      "cell_type": "code",
      "source": [
        "# PREDICTION FUNCTION --------------------------------------------------\n",
        "\n",
        "# function takes 39 days before first prediction day (input_seq)\n",
        "# then using encoder to identify hidden states for these 39 days.\n",
        "# Next, decoder takes hidden states provided by encoder\n",
        "# and predicts number of visitors from day 2 to day 40.\n",
        "# Day 40 is the first day of target_seq.\n",
        "\n",
        "# Predicted value for day 40 is appended to features of day 41.\n",
        "# Then function takes period from day 2 to day 40 and repeat the process\n",
        "# unil all days in target sequence get their predictions. \n",
        "\n",
        "# The output of the function is the vector with predictions that has\n",
        "# following shape: 820 restaurants * 39 days * 1 predicted visitors amount\n",
        "\n",
        "def predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n",
        "    # state of input sequence produced by encoder\n",
        "    state = inf_enc.predict(input_seq)\n",
        "    \n",
        "    # restrict target sequence to the same shape as X_lstm_test\n",
        "    target_seq = target_seq[:,:, :X_lstm_test.shape[2]]\n",
        "    \n",
        "    \n",
        "    # create vector that contains y for previous 7 days\n",
        "    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, X_lstm_test.shape[2]:-1]), axis = 2)\n",
        "    \n",
        "    # current sequence that is going to be modified each iteration of the prediction loop\n",
        "    current_seq = input_seq.copy()\n",
        "    \n",
        "    \n",
        "    # predicting outputs\n",
        "    output = np.ones([target_seq.shape[0],1,1])\n",
        "    for i in range(target_seq.shape[1]):\n",
        "        # add visitors for previous 7 days into features of a new day\n",
        "        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n",
        "        \n",
        "        # move prediction window one day forward\n",
        "        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n",
        "        \n",
        "        \n",
        "        # predict visitors amount\n",
        "        pred = inf_dec.predict([current_seq] + state)\n",
        "        \n",
        "        # update t_minus_seq\n",
        "        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n",
        "        t_minus_seq = t_minus_seq[:,:,:-1]        \n",
        "        \n",
        "        # update predicitons list\n",
        "        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n",
        "        \n",
        "        # update state\n",
        "        state = inf_enc.predict(current_seq)\n",
        "    \n",
        "    return output[:,1:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Jo3OyNbww1cY"
      },
      "cell_type": "code",
      "source": [
        "# INFERENCE ENCODER AND DECODER -----------------------------------------    \n",
        "# inference encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# inference decoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n",
        "                                                    initial_state = decoder_states_inputs))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                      [decoder_outputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "iTPRH7Uvw1cY"
      },
      "cell_type": "code",
      "source": [
        "# Predicting test values\n",
        "enc_dec_pred = predict_sequence(encoder_model,\n",
        "                                decoder_model,\n",
        "                                X_lstm[:,-X_lstm_test.shape[1]:,:],\n",
        "                                Y_lstm[:,-X_lstm_test.shape[1]:,:],\n",
        "                                X_lstm_test[:,:,:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "F95u1mQrw1cZ"
      },
      "cell_type": "code",
      "source": [
        "# test data\n",
        "sample_sub = pd.read_csv('sample_submission.csv')\n",
        "# transform test data\n",
        "air_test = sample_sub.copy()\n",
        "air_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])\n",
        "air_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])\n",
        "\n",
        "# dataframe for predictions\n",
        "submission_lstm = air_test.copy()\n",
        "submission_lstm.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kyHYnufNw1cZ"
      },
      "cell_type": "code",
      "source": [
        "# Add predicted test values to submission dataset ---------------------\n",
        "\n",
        "# Note: it is important to preserve the order of time series.\n",
        "# Thus, test set will contain all 829 lines in the same order as train set.\n",
        "# To make this 'air_store_id' is taken as in X and not in X_test (second line of 'test' variable below).\n",
        "# Only relevant results will be merged for submission dataframe\n",
        "test_df = df_to_reshape.loc[df_to_reshape['visit_date'].isin(test['visit_date'].values) &\n",
        "                         df_to_reshape['air_store_id'].isin(train_df['air_store_id'].values),]\n",
        "\n",
        "\n",
        "# reshape predicted values to initial shape\n",
        "test_pred = enc_dec_pred.reshape(test_df.shape[0], 1)\n",
        "test_pred_exp = np.exp(test_pred) - 1.0\n",
        "test_pred_exp[test_pred_exp<0] = 0\n",
        "\n",
        "# add predictions to dataframe with 'air_store_id' and 'visit_date'\n",
        "test_df_pred = test_df[['air_store_id', 'visit_date']].copy()\n",
        "test_df_pred['predicted'] = test_pred_exp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "L4ozGBP9w1cZ"
      },
      "cell_type": "code",
      "source": [
        "# reverse transform of 'air_store_id'\n",
        "test_df_pred['air_store_id'] = le_id.inverse_transform(test_df_pred['air_store_id'])\n",
        "\n",
        "# finalizing submission csv file\n",
        "submission_df = submission_lstm.merge(test_df_pred,\n",
        "                                     how = 'left',\n",
        "                                     left_on = ['air_store_id', 'visit_date'],\n",
        "                                     right_on = ['air_store_id', 'visit_date'])\n",
        "\n",
        "submission_df['visitors'] = submission_df['predicted']\n",
        "submission_df = submission_df.drop(['air_store_id', 'visit_date', 'predicted'], axis = 1)\n",
        "submission_df.to_csv('submission5.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}